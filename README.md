# ğŸš€ Pipeline de Dados: Airflow + Athena/S3 + Prometheus/Grafana

DemonstraÃ§Ã£o de pipeline moderno usando **Apache Airflow** para orquestrar ingestÃ£o, transformaÃ§Ã£o e observabilidade de dados na AWS. Ideal para entrevistas, POCs e estudos de arquitetura de dados com governanÃ§a e monitoramento.

---

## ğŸ“š SumÃ¡rio

- [VisÃ£o Geral](#visÃ£o-geral)
- [Arquitetura](#arquitetura)
- [Funcionalidades](#funcionalidades)
- [Estrutura do RepositÃ³rio](#estrutura-do-repositÃ³rio)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [ConfiguraÃ§Ã£o do Ambiente](#configuraÃ§Ã£o-do-ambiente)
- [ExecuÃ§Ã£o do Pipeline](#execuÃ§Ã£o-do-pipeline)
- [Observabilidade](#observabilidade)
- [GovernanÃ§a e SeguranÃ§a](#governanÃ§a-e-seguranÃ§a)
- [Troubleshooting](#troubleshooting)
- [PolÃ­tica IAM Recomendada](#polÃ­tica-iam-recomendada)
- [Roadmap](#roadmap)
- [Talk Track para Entrevista](#talk-track-para-entrevista)

---

## ğŸ¯ VisÃ£o Geral

Este projeto demonstra:

- **IngestÃ£o** de um CSV local para o S3 (raw)
- **CriaÃ§Ã£o de tabela externa** (Athena/Glue)
- **TransformaÃ§Ã£o CTAS** para Parquet particionado
- **Data Quality** (opcional)
- **Observabilidade** com Prometheus e Grafana
- **GovernanÃ§a**: IAM mÃ­nimo, Glue Catalog, boas prÃ¡ticas de seguranÃ§a

---

## ğŸ—ï¸ Arquitetura

```mermaid
graph TD
    subgraph Data
        CSV(CSV local)
    end

    subgraph Orchestration
        Airflow(Airflow)
    end

    subgraph AWS
        S3(S3: raw-data/ & processed-data/)
        Athena(Athena)
        Glue(Glue Catalog)
    end

    subgraph Monitoring
        StatsD(StatsD Exporter)
        Prometheus(Prometheus)
        Grafana(Grafana)
    end

    CSV --> Airflow
    Airflow -- Upload --> S3
    Airflow -- Create Table --> Athena
    Airflow -- CTAS --> Athena
    Athena -- Query --> S3
    Athena -- Catalog --> Glue
    Airflow -- Metrics --> StatsD
    StatsD --> Prometheus
    Prometheus --> Grafana
```

---

## âš™ï¸ Funcionalidades

- **DAG `orders_pipeline`** com as tasks:
    1. `upload_csv_to_s3`: envia `data/orders.csv` para `s3://<bucket>/raw-data/orders.csv`
    2. `ensure_glue_db`: cria o database no Glue se nÃ£o existir
    3. `create_external_raw_orders`: cria tabela externa sobre o CSV (Glue/Athena)
    4. `drop_orders_summary_if_exists`: garante idempotÃªncia do CTAS
    5. `clear_output_prefix`: limpa `processed-data/orders_summary/` antes de reprocessar
    6. `ctas_orders_summary`: CTAS no Athena para gerar Parquet em `processed-data/orders_summary/`
    7. *(Opcional)* Checagens de Data Quality (existÃªncia, nulos, contagem)
- **Observabilidade**: mÃ©tricas Airflow via StatsD â†’ Prometheus â†’ Grafana
- **GovernanÃ§a**: exemplos de IAM mÃ­nimo, uso do Glue Catalog, logs por task, recomendaÃ§Ãµes de seguranÃ§a

---

## ğŸ“ Estrutura do RepositÃ³rio

```
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ orders_pipeline.py      # DAG principal
â”œâ”€â”€ data/
â”‚   â””â”€â”€ orders.csv              # Base demo
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ airflow/                # Dockerfile + requirements do Airflow
â”œâ”€â”€ prometheus/
â”‚   â””â”€â”€ prometheus.yml          # ConfiguraÃ§Ã£o Prometheus
â”œâ”€â”€ grafana/                    # Dashboards/provisioning (opcional)
â”œâ”€â”€ .env                        # VariÃ¡veis locais (NÃƒO versionar)
â”œâ”€â”€ docker-compose.yml
```

---

## ğŸ“ PrÃ©-requisitos

1. **Bucket S3** na sua regiÃ£o (ex: `pt-data-lab` em `us-east-2`)
    - Prefixos: `raw-data/`, `processed-data/`, `athena-staging/`
2. **UsuÃ¡rio IAM** com Access Key/Secret e polÃ­tica mÃ­nima (veja [PolÃ­tica IAM Recomendada](#polÃ­tica-iam-recomendada))
3. **Docker** + **Docker Compose** instalados

---

## âš¡ ConfiguraÃ§Ã£o do Ambiente

1. **Crie o arquivo `.env`** (NÃƒO comite!):

    ```
    AWS_ACCESS_KEY_ID=...
    AWS_SECRET_ACCESS_KEY=...
    AWS_DEFAULT_REGION=us-east-2
    S3_BUCKET=pt-data-lab
    ```

2. **Build inicial (opcional):**
    ```bash
    docker compose build --no-cache
    ```

3. **Inicialize o Airflow:**
    ```bash
    docker compose up airflow-init
    ```

4. **Suba os serviÃ§os:**
    ```bash
    docker compose up -d
    ```

---

## ğŸš¦ ExecuÃ§Ã£o do Pipeline

1. Acesse o Airflow: [http://localhost:8080](http://localhost:8080)  
   *(user/pass padrÃ£o: airflow/airflow)*
2. Ative e execute a DAG `orders_pipeline`
3. Monitore as tasks e logs pela UI

---

## ğŸ“Š Observabilidade

- **Prometheus**: [http://localhost:9090](http://localhost:9090)
- **Grafana**: [http://localhost:3000](http://localhost:3000)
    - Configure a fonte Prometheus (`http://prometheus:9090`)
    - Importe dashboards de Airflow/StatsD ou use o provisioning em `grafana/`
- **MÃ©tricas Ãºteis**: latÃªncia de tasks, falhas/sucessos por DAG, duraÃ§Ã£o mÃ©dia, fila do scheduler

---

## ğŸ”’ GovernanÃ§a e SeguranÃ§a

- **CatÃ¡logo**: Glue Catalog como fonte de verdade para schemas/tabelas
- **SeguranÃ§a**:
    - IAM mÃ­nimo (veja abaixo)
    - Secret scan (gitleaks) no CI
    - `.env` no `.gitignore`
    - VariÃ¡veis sensÃ­veis fora do Git
- **Qualidade**:
    - Tasks de Data Quality prontas para habilitar
    - dbt-checks (se houver projeto dbt)

---

## ğŸ› ï¸ Troubleshooting

- **Logs detalhados**: disponÃ­veis por task na UI do Airflow
- **PermissÃµes S3/Athena**: verifique polÃ­tica IAM
- **Erros de conexÃ£o**: cheque variÃ¡veis no `.env` e status dos containers
- **Observabilidade**: valide targets em `prometheus/prometheus.yml` e dashboards no Grafana

---

## ğŸ“ PolÃ­tica IAM Recomendada

Exemplo mÃ­nimo para execuÃ§Ã£o do pipeline:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:ListBucket",
        "s3:DeleteObject"
      ],
      "Resource": [
        "arn:aws:s3:::pt-data-lab/*",
        "arn:aws:s3:::pt-data-lab"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "athena:*",
        "glue:*"
      ],
      "Resource": "*"
    }
  ]
}
```

---

## ğŸš§ Roadmap

- Ativar tasks de Data Quality como parte do pipeline
- Particionamento e compressÃ£o no CTAS (ex: snappy)
- IntegraÃ§Ã£o com dbt para modelagem incremental
- Linage (OpenLineage/Marquez) integrado ao Airflow
- Dashboards Grafana prontos via provisioning
- Release flow por tag com versionamento semÃ¢ntico

---

## ğŸ¤ Talk Track para Entrevista

- **Contexto**: OrquestraÃ§Ã£o com Airflow â†’ ingestÃ£o S3 â†’ modelagem via Athena (CTAS) â†’ Parquet otimizado
- **Por quÃª assim?**: Baixo acoplamento, custo reduzido (Athena/S3), catÃ¡logos centralizados (Glue)
- **Qualidade & GovernanÃ§a**: Checagens de DQ, IAM mÃ­nimo, secret scanning, branch protection
- **Observabilidade**: MÃ©tricas de DAG/task no Grafana; troubleshooting rÃ¡pido via Prometheus
- **Escalabilidade**: Particionamento, compaction, formatos colunares e modelo incremental (dbt)

---

> DÃºvidas ou sugestÃµes? Abra uma issue ou